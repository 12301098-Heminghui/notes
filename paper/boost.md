## Boosting related paper

#### 1. Experiments with a New Boosting Algorithm
1. AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing.
2. "pseudo-loss" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate.
3. "Boosting" is a general method for improving the performance of any learning algorithm.
4. Comparing boosting with bagging because both methods work by combining many classifiers.
5. Boosting generates a hypothesis whose error on the training set is small by combining many hypotheses whose error may be large.
6. Boosting has to do with variance reduction.
7. When starting with relatively simple classifiers, the improvement can be especially dramatic, and can often lead to a composite classifier that outperforms more complex "oneshot" learning algorithms like C4.5.
8. Note, however, that for non-binary classifcation problems, boosting simple classifiers can only be done effectively if the more sophisticated pseudo-loss is used.

#### 2. BoosTexter: A Boosting-based System for Text Categorizatio
